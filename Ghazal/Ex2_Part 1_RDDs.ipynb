{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "501bc548-f2b0-436d-a7a1-6e768c37239e",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Part 1) RDDs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bcbba45-e4e7-40bf-8cab-18e978b9dc9c",
   "metadata": {},
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1df19478-3a3e-4b67-b8e0-5bead0849d30",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://captain01.os.hpc.tuwien.ac.at:9999/proxy/application_1715326141961_2687\n",
       "SparkContext available as 'sc' (version = 3.2.3, master = yarn, app id = application_1715326141961_2687)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Path: String = hdfs:///user/dic24_shared/amazon-reviews/full/reviews_devset.json\n",
       "data: org.apache.spark.sql.DataFrame = [asin: string, category: string ... 8 more fields]\n",
       "pairs: org.apache.spark.rdd.RDD[(String, String)] = MapPartitionsRDD[9] at map at <console>:29\n",
       "stopwordsPath: String = Exercise2/stopwords.txt\n",
       "stopwords: Array[String] = Array(a, aa, able, about, above, absorbs, accord, according, accordingly, across, actually, after, afterwards, again, against, ain, album, album, all, allow, allows, almost, alone, along, already, also, although, always, am, among, amongst, an, and, another, any, anybody, anyhow, anyone, anything, anyway, anyways, anywhere, apart, app, appear, appreciate, appropriate, are, aren, around, as, aside, ask, asking, associated, at, available, away, awfully, ...\n"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val Path= \"hdfs:///user/dic24_shared/amazon-reviews/full/reviews_devset.json\"\n",
    "val data = spark.read.json(Path)\n",
    "// Making the RDD \n",
    "// Category is the Key \n",
    "// reviewText is the value\n",
    "val pairs = data.rdd.map{row=>\n",
    "    val category = row.getAs[String] (\"category\")\n",
    "    val text = row.getAs[String] (\"reviewText\")\n",
    "    (category,text)}\n",
    "// Reading stopWord into RDD string and convert it to an array\n",
    "val stopwordsPath = \"Exercise2/stopwords.txt\"\n",
    "val stopwords = sc.textFile(stopwordsPath).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab4659c7-e58e-4452-8886-e235811e63a6",
   "metadata": {},
   "source": [
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "86b28b8b-93c8-468b-9a70-21078c160d7a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "clean_reviews: org.apache.spark.rdd.RDD[(String, String)] = MapPartitionsRDD[12] at mapValues at <console>:28\n",
       "res0: Int = 2\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// converting the values(reviewText) to lowercase\n",
    "//spliting the reviewTexts into words and removing any non-alphabetic characters\n",
    "//removing stopWords from ReviewTexts\n",
    "val clean_reviews = pairs.mapValues(value=>value.toLowerCase\n",
    "                                  .split(\"[^a-zA-Z]+\")\n",
    "                                  .filterNot(x => stopwords.contains(x.toLowerCase)).mkString(\" \")).cache()\n",
    "// the number of partition is the defualt number \n",
    "//we tried different partitions and incsreasing the number of partitions resulted in increasing the time duration \n",
    "clean_reviews.getNumPartitions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d601d2e7-8034-44d2-b7ac-fcd1937f1e25",
   "metadata": {},
   "source": [
    "## Chi_2 Value Calculating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3e73d8bb-9482-4d3c-8174-7a92c5e14edf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "categoryTermCount: org.apache.spark.rdd.RDD[((String, String), Int)] = ShuffledRDD[15] at reduceByKey at <console>:25\n",
       "result_A: org.apache.spark.rdd.RDD[(String, (String, Int))] = MapPartitionsRDD[16] at map at <console>:27\n",
       "countWords: org.apache.spark.rdd.RDD[(String, Int)] = ShuffledRDD[20] at reduceByKey at <console>:29\n"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//counting number of each words per category\n",
    "val categoryTermCount = clean_reviews.flatMapValues(x=>x.split(\" \")).map(word => (word, 1)).reduceByKey ((x,y)=>x+y)\n",
    "// making a tuple with key value of category and value of (word,number of that word in the respective category)\n",
    "val result_A = categoryTermCount.map{case(k,v) => ( (k._2),( k._1, v))}\n",
    "// Calculate a number of unique words in the whole reviews. The result is a tuple (word, frequency of word )\n",
    "val countWords = clean_reviews.map{ case (key, value) =>  value }.flatMap (x=>x.split(\" \")).map(word => (word, 1)).reduceByKey ((x,y)=>x+y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dd00860d-efba-4dc6-ab25-a481bc7cb308",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rddJoin: org.apache.spark.rdd.RDD[(String, ((String, Int), Int))] = MapPartitionsRDD[23] at join at <console>:27\n",
       "result_A_B: org.apache.spark.rdd.RDD[(String, (String, Int, Int))] = MapPartitionsRDD[24] at map at <console>:29\n",
       "countCategory: org.apache.spark.rdd.RDD[(String, Int)] = ShuffledRDD[26] at reduceByKey at <console>:31\n"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Do the join datasets per word column\n",
    "val rddJoin = result_A.join(countWords)\n",
    "// Prepare data for the following join per category. The data now represents a tuple (category (word, A,B))\n",
    "val result_A_B = rddJoin.map{case (k,v)=> (v._1._1, (k, v._1._2, v._2-v._1._2)) }\n",
    "// Calculate a frequency of each category\n",
    "val countCategory = clean_reviews.map{ case (key, value) =>  (key,1)}.reduceByKey ((x,y)=>x+y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2775e982-8274-4119-98f8-1bb58e6395ee",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rddJoin2: org.apache.spark.rdd.RDD[(String, ((String, Int, Int), Int))] = MapPartitionsRDD[29] at join at <console>:26\n"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Join per category\n",
    "val rddJoin2 = result_A_B.join(countCategory).persist\n",
    "// Calculate the number of lines in the whole document\n",
    "\n",
    "// Calculate A,B,C,D\n",
    "// val A_B_C_D = rddJoin2.map{case (k,v)=> (k, (v._1._1, v._1._2, v._1._3, v._2-v._1._2,N - v._1._3 - v._2 )) }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "729c8835-ec1e-4a85-b5c6-8a5d2c530ea6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "N: Int = 78829\n",
       "chi2: org.apache.spark.rdd.RDD[(String, (Float, String))] = MapPartitionsRDD[30] at map at <console>:27\n",
       "grouped: org.apache.spark.rdd.RDD[(String, List[(Float, String)])] = MapPartitionsRDD[32] at mapValues at <console>:36\n",
       "grouped_75: org.apache.spark.rdd.RDD[(String, List[(Float, String)])] = ShuffledRDD[36] at sortByKey at <console>:38\n"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Calculate chi_2 value using A,B,C,D,N\n",
    "val N = clean_reviews.count().toInt\n",
    "val chi2 = rddJoin2.map{case (k,v)=> {\n",
    "    val A = v._1._2.toFloat\n",
    "    val B = v._1._3.toFloat\n",
    "    val C = v._2-v._1._2.toFloat\n",
    "    val D = N - v._1._3 - v._2.toFloat\n",
    "    val result = (N*(A*D-B*C)*(A*D-B*C))/((A+B)*(A+C)*(B+D)*(C+D))\n",
    "    (k, (result,v._1._1))\n",
    "}}\n",
    "// Group the lines according to the key (=category) and sort according to the value of chi_2\n",
    "val grouped = chi2.groupByKey().mapValues(tuple => tuple.toList.sortBy(-_._1))\n",
    "// Extract the first 75 values in each category\n",
    "val grouped_75 = grouped.mapValues(line=>line.take(75)).sortByKey()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea54471f-660c-46b9-858c-e56a38368404",
   "metadata": {},
   "source": [
    "## Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "adaaaa70-aa80-48e6-9348-223f3e220471",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import scala.reflect.io.File\n",
       "output: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[42] at map at <console>:32\n",
       "file: scala.reflect.io.File = output_rdd.txt\n",
       "terms: Array[String] = Array(abq, acdelco, acne, acoustic, acre, acting, action, actor, actors, actress, acura, adapter, addario, addicted, addicting, addictive, ads, ai, aic, aidan, aimard, aired, airsoft, albums, alex, almonds, alpha, alternator, altima, ameda, amel, amino, ammo, amp, android, animated, animation, anime, answering, antenna, ants, apos, appetite, apple, applecare, apply, apps, aquarium, ar, arai, arch, arrangements, articulation, artisan, artist, artists, atdi, atv, audio, author, authors, avent, awesome, babies, babyface, back, backpacking, bag, bags, bait, baits, baking, ball, ballad, ballads, ballmount, ball...\n"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import scala.reflect.io.File\n",
    "\n",
    "// Transforming and saving the output like the previous exercise\n",
    "val output = grouped_75.map { case (category, terms) =>\n",
    "  val formattedTerms = terms.map { case (term, chi2) =>\n",
    "    s\"$chi2:$term\"\n",
    "  }.mkString(\" \")\n",
    "\n",
    "  s\"<$category> $formattedTerms\"\n",
    "}\n",
    "\n",
    "// Save the output\n",
    "val file = File(\"output_rdd.txt\")\n",
    "file.writeAll(output.collect().mkString(\"\\n\"))\n",
    "\n",
    "// Extract terms and sort them alphabetically\n",
    "val terms = grouped_75.flatMap { case (category, terms) =>\n",
    "  terms.map { case (term, chi2) => chi2 }\n",
    "}.distinct().collect().sorted\n",
    "\n",
    "// Append the sorted terms to the file\n",
    "file.appendAll(\"\\n\" + terms.mkString(\" \"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c68dc80-f2ac-4b55-b39c-4b91f182c97f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
